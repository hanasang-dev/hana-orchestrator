ktor {
    deployment {
        port = 8080
        port = ${?PORT}
    }
    
    application {
        modules = [ com.hana.orchestrator.ApplicationKt.main ]
    }
}

# LLM 설정
# 확장성: 각 복잡도별로 다른 프로바이더 사용 가능 (예: SIMPLE=OLLAMA, MEDIUM=OPENAI)
# 
# 각 복잡도별로 독립적인 설정:
# - SIMPLE 작업: simpleProvider, simpleModelId, simpleBaseUrl 사용 (simpleApiKey는 환경변수로만)
# - MEDIUM 작업: mediumProvider, mediumModelId, mediumBaseUrl 사용 (mediumApiKey는 환경변수로만)
# - COMPLEX 작업: complexProvider, complexModelId, complexBaseUrl 사용 (complexApiKey는 환경변수로만)
#
# 예시: MEDIUM 작업에 OpenAI 사용
#   export LLM_MEDIUM_PROVIDER=OPENAI
#   export LLM_MEDIUM_MODEL=gpt-4
#   export LLM_MEDIUM_BASE_URL=https://api.openai.com/v1
#   export LLM_MEDIUM_API_KEY=sk-...
llm {
    # 간단한 작업용 모델 (validateQueryFeasibility, extractParameters)
    # 2026년 기준 최신 추천: Gemma 2 2B - M3 Pro에서 18 tokens/sec, 3.2GB 메모리, 가장 빠름
    # 대안: Phi-3 Mini 3.8B (14 tok/s, 추론 능력 우수), Llama 3.2 3B (128K 컨텍스트)
    simple {
        provider = "OLLAMA"
        provider = ${?LLM_SIMPLE_PROVIDER}
        modelId = "gemma2:2b"
        modelId = ${?LLM_SIMPLE_MODEL}
        contextLength = 8192
        contextLength = ${?LLM_SIMPLE_CONTEXT}
        baseUrl = "http://localhost:11434"
        baseUrl = ${?LLM_SIMPLE_BASE_URL}
        # API 키는 보안상 환경변수로만 설정: LLM_SIMPLE_API_KEY
    }
    
    # 중간 작업용 모델 (evaluateResult, compareExecutions)
    # 2026년 기준 추천: Llama 3.1 8B - M3 Pro에서 28 tok/s, 안정적이고 빠른 성능
    # 대안: Llama 3.2 11B (더 큰 컨텍스트), Qwen2.5 7B (M1 Pro 25-30 tok/s)
    # 클라우드 API 사용 예시:
    #   export LLM_MEDIUM_PROVIDER=OPENAI
    #   export LLM_MEDIUM_MODEL=gpt-4
    #   export LLM_MEDIUM_BASE_URL=https://api.openai.com/v1
    #   export LLM_MEDIUM_API_KEY=sk-...
    medium {
        provider = "OLLAMA"
        provider = ${?LLM_MEDIUM_PROVIDER}
        modelId = "llama3.1:8b"
        modelId = ${?LLM_MEDIUM_MODEL}
        contextLength = 128000
        contextLength = ${?LLM_MEDIUM_CONTEXT}
        baseUrl = "http://localhost:11434"
        baseUrl = ${?LLM_MEDIUM_BASE_URL}
        # API 키는 보안상 환경변수로만 설정: LLM_MEDIUM_API_KEY
    }
    
    # 복잡한 작업용 모델 (createExecutionTree, suggestRetryStrategy)
    # 2026년 기준: Llama 3.1 8B 사용 (llama3.2:11b는 일반 텍스트 모델로 존재하지 않음)
    # 향후 더 큰 모델 필요 시: llama3.1:70b 또는 다른 대형 모델 고려 가능
    complex {
        provider = "OLLAMA"
        provider = ${?LLM_COMPLEX_PROVIDER}
        modelId = "llama3.1:8b"
        modelId = ${?LLM_COMPLEX_MODEL}
        contextLength = 128000
        contextLength = ${?LLM_COMPLEX_CONTEXT}
        baseUrl = "http://localhost:11434"
        baseUrl = ${?LLM_COMPLEX_BASE_URL}
        # API 키는 보안상 환경변수로만 설정: LLM_COMPLEX_API_KEY
    }
    
    # 공통 설정
    timeoutMs = 120000
    timeoutMs = ${?LLM_TIMEOUT_MS}
}
