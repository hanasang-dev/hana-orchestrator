ktor {
    deployment {
        port = 8080
        port = ${?PORT}
    }
    
    application {
        modules = [ com.hana.orchestrator.ApplicationKt.main ]
    }
}

# LLM 설정
# 확장성: 각 복잡도별로 다른 프로바이더 사용 가능 (예: SIMPLE=OLLAMA, MEDIUM=OPENAI)
# 
# 각 복잡도별로 독립적인 설정:
# - SIMPLE 작업: simpleProvider, simpleModelId, simpleBaseUrl 사용 (simpleApiKey는 환경변수로만)
# - MEDIUM 작업: mediumProvider, mediumModelId, mediumBaseUrl 사용 (mediumApiKey는 환경변수로만)
# - COMPLEX 작업: complexProvider, complexModelId, complexBaseUrl 사용 (complexApiKey는 환경변수로만)
#
# 예시: MEDIUM 작업에 OpenAI 사용
#   export LLM_MEDIUM_PROVIDER=OPENAI
#   export LLM_MEDIUM_MODEL=gpt-4
#   export LLM_MEDIUM_BASE_URL=https://api.openai.com/v1
#   export LLM_MEDIUM_API_KEY=sk-...
#
# 다국어(한국어) 강한 모델 추천 (2026년 기준, Ollama)
# - 속도 유지(지금 수준): exaone3.5:2.4b (LG, 한영 2.4B, 1.6GB) — ollama pull exaone3.5:2.4b
# - 품질 우선(조금 느림): exaone3.5:7.8b (한영 7.8B), qwen3:8b (다국어 100+ 언어, 2025년 4월 Qwen3 출시)
# - COMPLEX만 바꿀 때: LLM_COMPLEX_MODEL=exaone3.5:2.4b 또는 LLM_COMPLEX_MODEL=exaone3.5:7.8b
#
# 타협점 구성 (속도 + 품질): SIMPLE=2.4B(빠름), MEDIUM/COMPLEX=7.8B(트리·평가 안정)
# 필요: ollama pull exaone3.5:2.4b && ollama pull exaone3.5:7.8b
llm {
    # 간단한 작업 (extractParameters, checkIfLLMCanAnswerDirectly) — 속도 우선
    simple {
        provider = "OLLAMA"
        provider = ${?LLM_SIMPLE_PROVIDER}
        modelId = "exaone3.5:2.4b"
        modelId = ${?LLM_SIMPLE_MODEL}
        contextLength = 8192
        contextLength = ${?LLM_SIMPLE_CONTEXT}
        baseUrl = "http://localhost:11434"
        baseUrl = ${?LLM_SIMPLE_BASE_URL}
        # API 키는 보안상 환경변수로만 설정: LLM_SIMPLE_API_KEY
    }
    
    # 중간 작업 (evaluateResult, compareExecutions, generateDirectAnswer) — 품질
    medium {
        provider = "OLLAMA"
        provider = ${?LLM_MEDIUM_PROVIDER}
        modelId = "exaone3.5:7.8b"
        modelId = ${?LLM_MEDIUM_MODEL}
        contextLength = 8192
        contextLength = ${?LLM_MEDIUM_CONTEXT}
        baseUrl = "http://localhost:11434"
        baseUrl = ${?LLM_MEDIUM_BASE_URL}
        # API 키는 보안상 환경변수로만 설정: LLM_MEDIUM_API_KEY
    }
    
    # 복잡한 작업 (createExecutionTree, suggestRetryStrategy) — 트리 구성 안정
    complex {
        provider = "OLLAMA"
        provider = ${?LLM_COMPLEX_PROVIDER}
        modelId = "exaone3.5:7.8b"
        modelId = ${?LLM_COMPLEX_MODEL}
        contextLength = 8192
        contextLength = ${?LLM_COMPLEX_CONTEXT}
        baseUrl = "http://localhost:11434"
        baseUrl = ${?LLM_COMPLEX_BASE_URL}
        # API 키는 보안상 환경변수로만 설정: LLM_COMPLEX_API_KEY
    }
    
    # 공통 설정
    timeoutMs = 120000
    timeoutMs = ${?LLM_TIMEOUT_MS}
}
