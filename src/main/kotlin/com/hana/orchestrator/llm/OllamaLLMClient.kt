package com.hana.orchestrator.llm

import ai.koog.prompt.executor.ollama.client.OllamaClient
import ai.koog.prompt.executor.clients.ConnectionTimeoutConfig
import ai.koog.prompt.llm.LLMCapability
import ai.koog.prompt.llm.LLMProvider
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.dsl.Prompt
import ai.koog.prompt.message.Message
import com.hana.orchestrator.domain.entity.ExecutionNode
import com.hana.orchestrator.domain.entity.ExecutionTree
import com.hana.orchestrator.domain.entity.ExecutionHistory
import com.hana.orchestrator.data.model.response.ExecutionTreeResponse
import com.hana.orchestrator.data.mapper.ExecutionTreeMapper
import com.hana.orchestrator.context.AppContextService
import com.hana.orchestrator.context.DefaultPromptComposer
import com.hana.orchestrator.context.LLMTaskType
import com.hana.orchestrator.context.PromptComposer
import com.hana.orchestrator.llm.config.LLMConfig
import com.hana.orchestrator.layer.LayerDescription
import com.hana.orchestrator.orchestrator.createOrchestratorLogger
import kotlinx.serialization.json.Json
import kotlinx.coroutines.withTimeout
import ai.koog.prompt.params.LLMParams
import kotlinx.serialization.json.JsonObject

/**
 * Ollama ë¡œì»¬ LLM í†µì‹ ì„ ìœ„í•œ ëª¨ë“ˆ
 * SRP: LLM í†µì‹  ë° ì‘ë‹µ íŒŒì‹±ë§Œ ë‹´ë‹¹
 * 
 * OOP ì›ì¹™:
 * - SRP: LLM í†µì‹  ì±…ì„ë§Œ ë‹´ë‹¹, í”„ë¡¬í”„íŠ¸ ìƒì„±/í´ë°± ì²˜ë¦¬ëŠ” ë³„ë„ í´ë˜ìŠ¤ë¡œ ë¶„ë¦¬
 * - DRY: ê³µí†µ LLM í˜¸ì¶œ ë¡œì§ ì¶”ì¶œ
 * - ìº¡ìŠí™”: ë‚´ë¶€ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ ìˆ¨ê¹€
 * - DIP: LLMClient ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
 */
class OllamaLLMClient(
    private val modelId: String = "qwen3:8b",
    private val contextLength: Long = 40_960L,
    private val timeoutMs: Long = 120_000L,
    private val baseUrl: String = "http://localhost:11434"
) : LLMClient {
    private val logger = createOrchestratorLogger(OllamaLLMClient::class.java, null)
    
    /**
     * ì„¤ì • ê¸°ë°˜ ìƒì„±ì
     */
    constructor(config: LLMConfig, modelId: String, contextLength: Long, baseUrl: String) : this(
        modelId = modelId,
        contextLength = contextLength,
        timeoutMs = config.timeoutMs,
        baseUrl = baseUrl
    )
    // ê³µí†µ JSON ì„¤ì • (ìº¡ìŠí™”)
    private val jsonConfig = Json { 
        ignoreUnknownKeys = true
        isLenient = true
        coerceInputValues = true
    }
    
    // Ollama í´ë¼ì´ì–¸íŠ¸ (baseUrl ì„¤ì • ê°€ëŠ¥)
    // ai.koogì˜ simpleOllamaAIExecutor()ëŠ” í™˜ê²½ë³€ìˆ˜ OLLAMA_HOSTë¥¼ ì½ì§€ë§Œ,
    // ëŸ°íƒ€ì„ì— í™˜ê²½ë³€ìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ OllamaClientë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ ì‚¬ìš©
    // OllamaClient ìƒì„±ìì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ê°€ baseUrl: String
    // íƒ€ì„ì•„ì›ƒ ì„¤ì •: ì ì ˆí•œ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì„¤ì •
    private val ollamaClient = OllamaClient(
        baseUrl = baseUrl,
        timeoutConfig = ConnectionTimeoutConfig(
            connectTimeoutMillis = 5_000L,       // ì—°ê²° íƒ€ì„ì•„ì›ƒ: 5ì´ˆ
            requestTimeoutMillis = timeoutMs,     // ìš”ì²­ íƒ€ì„ì•„ì›ƒ: ì„¤ì •ê°’ ì‚¬ìš©
            socketTimeoutMillis = timeoutMs      // ì†Œì¼“ íƒ€ì„ì•„ì›ƒ: ìš”ì²­ íƒ€ì„ì•„ì›ƒê³¼ ë™ì¼
        )
    )
    
    // í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° (SRP: í”„ë¡¬í”„íŠ¸ ìƒì„± ì±…ì„ ë¶„ë¦¬)
    private val promptBuilder = LLMPromptBuilder()
    private val promptComposer: PromptComposer = DefaultPromptComposer()
    
    /**
     * ê³µí†µ LLM ëª¨ë¸ ìƒì„±
     * DRY: ë°˜ë³µë˜ëŠ” ëª¨ë¸ ìƒì„± ë¡œì§ ê³µí†µí™”
     */
    private fun createLLMModel(): LLModel {
        return LLModel(
            provider = LLMProvider.Ollama,
            id = modelId,
            capabilities = listOf(
                LLMCapability.Temperature,
                LLMCapability.Schema.JSON.Basic,
                LLMCapability.Tools
            ),
            contextLength = contextLength
        )
    }
    
    /**
     * ê³µí†µ LLM í˜¸ì¶œ ë¡œì§
     * JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ í¬í•¨ (ìµœëŒ€ 2íšŒ ì¬ì‹œë„)
     * 2026ë…„ ê¸°ì¤€: Ollama Structured Outputsë¥¼ í™œìš©í•˜ì—¬ ìŠ¤í‚¤ë§ˆë¥¼ ê°•ì œ
     * - í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ë¯€ë¡œ format íŒŒë¼ë¯¸í„°ë¡œ ìŠ¤í‚¤ë§ˆ ì „ë‹¬
     * - íŒŒì‹± ì‹¤íŒ¨ ì‹œ í”„ë¡¬í”„íŠ¸ì— ì—ëŸ¬ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì¬ìš”ì²­
     */
    private suspend fun <T> callLLM(
        prompt: String,
        responseParser: (String) -> T,
        schema: JsonObject? = null,
        timeoutMs: Long = this.timeoutMs,
        maxRetries: Int = 2,
        logRawJson: Boolean = false
    ): T {
        var lastError: Exception? = null
        var lastJsonText: String? = null
        
        repeat(maxRetries + 1) { attempt ->
            try {
                val model = createLLMModel()
                val enhancedPrompt = if (attempt > 0 && lastError != null) {
                    // ì¬ì‹œë„ ì‹œ: ì´ì „ ì—ëŸ¬ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                    """
                    $prompt
                    
                    ì¤‘ìš”: ì´ì „ ì‘ë‹µì—ì„œ JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
                    ì˜¤ë¥˜: ${lastError?.message}
                    ì´ì „ ì‘ë‹µ: ${lastJsonText?.take(200) ?: "ì—†ìŒ"}
                    
                    ë°˜ë“œì‹œ ì™„ì „í•˜ê³  ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ëª¨ë“  í•„ìˆ˜ í•„ë“œë¥¼ í¬í•¨í•˜ê³ , ë”°ì˜´í‘œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì´ìŠ¤ì¼€ì´í”„í•˜ì„¸ìš”.
                    """.trimIndent()
                } else {
                    prompt
                }
                
                // TODO: Ollama Structured Outputs ì§€ì› ì¶”ê°€ í•„ìš”
                // í˜„ì¬ëŠ” í”„ë¡¬í”„íŠ¸ì— ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
                // schema íŒŒë¼ë¯¸í„°ëŠ” ì¤€ë¹„ë˜ì—ˆì§€ë§Œ LLMParams.Schema.JSON ìƒì„± ë°©ë²• í™•ì¸ í•„ìš”
                val promptDsl = Prompt.build(id = "llm-call") {
                    user(enhancedPrompt)
                    // í–¥í›„ ì¶”ê°€: params { schema = LLMParams.Schema.JSON(schema) }
                }
                
                val responses = withTimeout(timeoutMs) {
                    ollamaClient.execute(
                        prompt = promptDsl,
                        model = model,
                        tools = emptyList()
                    )
                }
                
                val responseText = when (val firstResponse = responses.firstOrNull()) {
                    is Message.Assistant -> firstResponse.content
                    is Message.Tool.Call -> firstResponse.content
                    else -> throw Exception("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                }
                
                val jsonText = JsonExtractor.extract(responseText)
                lastJsonText = jsonText
                if (logRawJson) {
                    logger.info("ğŸ“‹ [íŠ¸ë¦¬ìƒì„±] LLM ì›ë³¸ ë°˜í™˜ (ì§ì ‘ ì¶œë ¥):\n$jsonText")
                }

                return responseParser(jsonText)
            } catch (e: Exception) {
                lastError = e
                if (attempt < maxRetries) {
                    // ì¬ì‹œë„ ì „ì— ì ì‹œ ëŒ€ê¸° (LLMì´ ì´ì „ ì‘ë‹µì„ ì²˜ë¦¬í•  ì‹œê°„ ì œê³µ)
                    kotlinx.coroutines.delay(500)
                } else {
                    // ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                    throw Exception("LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ (${maxRetries + 1}íšŒ ì‹œë„): ${e.message}", e)
                }
            }
        }
        
        // ì´ë¡ ì ìœ¼ë¡œ ë„ë‹¬ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ ì»´íŒŒì¼ëŸ¬ë¥¼ ìœ„í•´
        throw Exception("LLM í˜¸ì¶œ ì‹¤íŒ¨")
    }
    
    /**
     * ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë ˆì´ì–´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ExecutionTree êµ¬ì¡°ì˜ ì‹¤í–‰ ê³„íšì„ ìƒì„±
     */
    /** íŠ¸ë¦¬ ìƒì„± ì „ìš© íƒ€ì„ì•„ì›ƒ(ms). ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ì—ì„œ LLMì´ ì˜¤ë˜ ê±¸ë¦´ ë•Œ ì „ì²´ ìš”ì²­ì´ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ ì•Šë„ë¡ ì œí•œ. */
    private val treeCreationTimeoutMs = minOf(timeoutMs, 60_000L)

    /** ìƒì„±ëœ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë¡œê·¸ë¡œ ë‚¨ê²¨, ì‚¬ìš©ì ìš”êµ¬ ëŒ€ë¹„ LLMì´ ë­˜ ë°˜í™˜í–ˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥í•˜ê²Œ í•¨. */
    private fun logCreatedTreeStructure(tree: ExecutionTree) {
        val totalNodes = tree.allNodes().size
        val chain = tree.rootNodes.joinToString(" â†’ ") { root ->
            sequence {
                var n: ExecutionNode? = root
                while (n != null) {
                    yield("${n.layerName}.${n.function}")
                    n = n.children.firstOrNull()
                }
            }.toList().joinToString(" â†’ ")
        }
        logger.info("ğŸ“‹ [íŠ¸ë¦¬ìƒì„±] LLMì´ ë°˜í™˜í•œ íŠ¸ë¦¬: ë£¨íŠ¸ ${tree.rootNodes.size}ê°œ, ì´ ë…¸ë“œ ${totalNodes}ê°œ | ì²´ì¸: $chain")
    }

    override suspend fun createExecutionTree(
        userQuery: String,
        layerDescriptions: List<LayerDescription>,
        appContextService: AppContextService?
    ): ExecutionTree {
        val body = promptBuilder.buildExecutionTreePrompt(userQuery, layerDescriptions)
        val prompt = if (appContextService != null) {
            promptComposer.compose(LLMTaskType.CREATE_TREE, appContextService, body)
        } else {
            body
        }
        logger.info("ğŸ“‹ [íŠ¸ë¦¬ìƒì„±] LLM í”„ë¡¬í”„íŠ¸ ê¸¸ì´: ${prompt.length}ì")
        logger.info("ğŸ“‹ [íŠ¸ë¦¬ìƒì„±] LLM í”„ë¡¬í”„íŠ¸:\n$prompt")
        val availableLayerNames = layerDescriptions.map { it.name }
        val schema = JsonSchemaBuilder.buildExecutionTreeSchema(availableLayerNames)
        
        logger.info("ğŸ”„ [íŠ¸ë¦¬ìƒì„±] LLM í˜¸ì¶œ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: ${treeCreationTimeoutMs}ms)")
        return try {
            val tree = callLLM(
                prompt = prompt,
                schema = schema,
                responseParser = { jsonText ->
                    val treeResponse = jsonConfig.decodeFromString<ExecutionTreeResponse>(jsonText)
                    ExecutionTreeMapper.toExecutionTree(treeResponse)
                },
                timeoutMs = treeCreationTimeoutMs,
                logRawJson = true
            )
            logger.info("âœ… [íŠ¸ë¦¬ìƒì„±] LLM í˜¸ì¶œ ì™„ë£Œ")
            logCreatedTreeStructure(tree)
            tree
        } catch (e: Exception) {
            logger.error("âŒ [íŠ¸ë¦¬ìƒì„±] LLM í˜¸ì¶œ ì‹¤íŒ¨: ${e.message}")
            throw e
        }
    }
    
    override suspend fun evaluateResult(
        userQuery: String,
        executionResult: String,
        executionSummary: String?
    ): ResultEvaluation {
        val prompt = promptBuilder.buildEvaluationPrompt(userQuery, executionResult, executionSummary)
        logger.info("ğŸ“‹ [í‰ê°€] LLMì— ë„˜ê¸°ëŠ” í”„ë¡¬í”„íŠ¸:\n$prompt")
        val schema = JsonSchemaBuilder.buildResultEvaluationSchema()
        
        return callLLM(
            prompt = prompt,
            schema = schema,
            responseParser = { jsonText ->
                jsonConfig.decodeFromString<ResultEvaluation>(jsonText)
            }
        )
    }
    
    /**
     * ì‹¤íŒ¨í•œ ì‹¤í–‰ì— ëŒ€í•œ ì¬ì²˜ë¦¬ ë°©ì•ˆì„ LLMì´ ì œì‹œ
     */
    override suspend fun suggestRetryStrategy(
        userQuery: String,
        previousHistory: ExecutionHistory,
        layerDescriptions: List<LayerDescription>,
        previousExecutedWorkSummary: String?
    ): RetryStrategy {
        val prompt = promptBuilder.buildRetryStrategyPrompt(userQuery, previousHistory, layerDescriptions, previousExecutedWorkSummary)
        val availableLayerNames = layerDescriptions.map { it.name }
        val schema = JsonSchemaBuilder.buildRetryStrategySchema(availableLayerNames)
        
        return callLLM(
            prompt = prompt,
            schema = schema,
            responseParser = { jsonText ->
                // ë°°ì—´ë¡œ ë°˜í™˜ëœ ê²½ìš° ì²˜ë¦¬ (ì˜ˆ: [{"layerName":...}] -> {"newTree": {"rootNodes": [...]}})
                val normalizedJson = if (jsonText.trimStart().startsWith("[")) {
                    // ë°°ì—´ì„ ê°ì²´ë¡œ ë³€í™˜
                    val arrayContent = jsonText.trim().removeSurrounding("[", "]")
                    """{"shouldStop":false,"reason":"ì¬ì²˜ë¦¬ ë°©ì•ˆ","newTree":{"rootNodes":[$arrayContent]}}"""
                } else {
                    jsonText
                }
                val retryResponse = jsonConfig.decodeFromString<RetryStrategyResponse>(normalizedJson)
                parseRetryStrategy(retryResponse)
            }
        )
    }
    
    /**
     * ì¬ì²˜ë¦¬ ë°©ì•ˆ íŒŒì‹±
     * SRP: íŒŒì‹± ë¡œì§ ë¶„ë¦¬
     */
    private fun parseRetryStrategy(retryResponse: RetryStrategyResponse): RetryStrategy {
        return if (retryResponse.shouldStop || retryResponse.newTree == null) {
            RetryStrategy(
                shouldStop = true,
                reason = retryResponse.reason.ifEmpty { "ì¬ì²˜ë¦¬ ë¶ˆê°€ëŠ¥" },
                newTree = null
            )
        } else {
            val newTree = ExecutionTreeMapper.toExecutionTree(retryResponse.newTree)
            RetryStrategy(
                shouldStop = false,
                reason = retryResponse.reason.ifEmpty { "ì¬ì²˜ë¦¬ ë°©ì•ˆ ì œì‹œ" },
                newTree = newTree
            )
        }
    }
    
    /**
     * ì´ì „ ì‹¤í–‰ê³¼ í˜„ì¬ ì‹¤í–‰ì„ ë¹„êµí•˜ì—¬ ìœ ì˜ë¯¸í•œ ë³€ê²½ì´ ìˆëŠ”ì§€ LLMì´ íŒë‹¨
     */
    override suspend fun compareExecutions(
        userQuery: String,
        previousTree: ExecutionTree?,
        previousResult: String,
        currentTree: ExecutionTree,
        currentResult: String
    ): ComparisonResult {
        val prompt = promptBuilder.buildComparisonPrompt(
            userQuery, previousTree, previousResult, currentTree, currentResult
        )
        val schema = JsonSchemaBuilder.buildComparisonResultSchema()
        
        return callLLM(
            prompt = prompt,
            schema = schema,
            responseParser = { jsonText ->
                jsonConfig.decodeFromString<ComparisonResult>(jsonText)
            }
        )
    }
    
    /**
     * ë¶€ëª¨ ë ˆì´ì–´ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ìì‹ ë ˆì´ì–´ í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜
     * ì˜ˆ: íŒŒì¼ìƒì„± ë ˆì´ì–´ê°€ "file.txt" ë°˜í™˜ -> ì¸ì½”ë”© ë ˆì´ì–´ì˜ "filePath" íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜
     */
    override suspend fun extractParameters(
        parentResult: String,
        childLayerName: String,
        childFunctionName: String,
        childFunctionDetails: com.hana.orchestrator.layer.FunctionDescription,
        layerDescriptions: List<LayerDescription>
    ): Map<String, Any> {
        val prompt = promptBuilder.buildParameterExtractionPrompt(
            parentResult = parentResult,
            childLayerName = childLayerName,
            childFunctionName = childFunctionName,
            childFunctionDetails = childFunctionDetails,
            layerDescriptions = layerDescriptions
        )
        
        return callLLM(
            prompt = prompt,
            responseParser = { jsonText ->
                val paramsResponse = jsonConfig.decodeFromString<Map<String, String>>(jsonText)
                // Stringì„ ì ì ˆí•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                paramsResponse.mapValues { (key, value) ->
                    val paramInfo = childFunctionDetails.parameters[key]
                    when (paramInfo?.type) {
                        "Int", "kotlin.Int" -> value.toIntOrNull() ?: value
                        "Long", "kotlin.Long" -> value.toLongOrNull() ?: value
                        "Double", "kotlin.Double" -> value.toDoubleOrNull() ?: value
                        "Boolean", "kotlin.Boolean" -> value.toBooleanStrictOrNull() ?: value
                        else -> value // String ë˜ëŠ” ê¸°íƒ€
                    }
                }
            }
        )
    }
    
    /**
     * ë ˆì´ì–´ë¡œ ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ìš”ì²­ì— ëŒ€í•´ LLMì´ ì§ì ‘ ë‹µë³€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
     */
    override suspend fun checkIfLLMCanAnswerDirectly(userQuery: String): LLMDirectAnswerCapability {
        val prompt = promptBuilder.buildLLMDirectAnswerCapabilityPrompt(userQuery)
        val schema = JsonSchemaBuilder.buildLLMDirectAnswerCapabilitySchema()
        
        return callLLM(
            prompt = prompt,
            schema = schema,
            responseParser = { jsonText ->
                jsonConfig.decodeFromString<LLMDirectAnswerCapability>(jsonText)
            }
        )
    }
    
    /**
     * LLMì´ ì§ì ‘ ë‹µë³€ ìƒì„± (ë ˆì´ì–´ ì—†ì´)
     */
    override suspend fun generateDirectAnswer(userQuery: String): String {
        val prompt = promptBuilder.buildDirectAnswerPrompt(userQuery)
        
        val model = createLLMModel()
        val promptDsl = Prompt.build(id = "direct-answer") {
            user(prompt)
        }
        
        val responses = withTimeout(timeoutMs) {
            ollamaClient.execute(
                prompt = promptDsl,
                model = model,
                tools = emptyList()
            )
        }
        
        val responseText = when (val firstResponse = responses.firstOrNull()) {
            is Message.Assistant -> firstResponse.content
            is Message.Tool.Call -> firstResponse.content
            else -> throw Exception("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        }
        
        return responseText.trim()
    }
    
    override suspend fun close() {
        // Ollama í´ë¼ì´ì–¸íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        ollamaClient.close()
    }
}
