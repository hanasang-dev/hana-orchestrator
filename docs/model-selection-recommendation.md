# 모델 선택 추천 (2026년 기준)

## 2026년 현재 기준 최신 모델 리스트 (Ollama / 오픈가중치)

아래는 2025~2026년 벤치마크·리더보드(Open LLM Leaderboard, Open Ko-LLM Leaderboard, Ollama 인기·최신)를 반영한 **규모별·용도별** 추천이다. Ollama 모델명 기준.

### 규모별 대표 모델 (Ollama)

| 규모 | Ollama 모델명 | 비고 |
|------|----------------|------|
| **~2B** | `exaone3.5:2.4b`, `gemma2:2b`, `qwen3:1.7b` | exaone 한영 강함. gemma2·qwen3 경량. |
| **~3B** | `qwen2.5:3b`, `qwen3:4b` | Open LLM 리더보드 2B대 상위. |
| **7B** | `mistral:7b`, `qwen2.5:7b` | 속도·다국어. 7B대 벤치 상위. |
| **8B** | `llama3.1:8b`, `qwen3:8b`, `exaone3.5:7.8b` | 범용·다국어·한국어. 8B대 표준. |
| **14B** | `qwen3:14b`, `qwen2.5:14b` | 14B대 벤치 상위. 다국어. |
| **32B** | `llama3.1:32b`, `qwen3:32b` | 고품질 추론. 리소스 여유 시. |
| **70B** | `llama3.3:70b`, `llama3.1:70b` | 70B대 SOTA. 도구 사용·추론 강함. |

### 2025~2026년 새로 올라온/강조되는 계열

| 계열 | Ollama 예시 | 특징 |
|------|-------------|------|
| **Qwen3** | `qwen3:8b`, `qwen3:14b`, `qwen3:32b` | 2025년 4월 출시. 100+ 언어, thinking/non-thinking 모드. |
| **Llama 3.3** | `llama3.3:70b` | Meta. 128K 컨텍스트, 70B로 405B급 성능 근접. |
| **DeepSeek R1** | `deepseek-r1:8b` 등 | 오픈 추론 모델. 수학·코드·논리 강함. |
| **Gemma 3** | `gemma3:*` | Google. 단일 GPU 최상급 옵션. |
| **EXAONE 3.5** | `exaone3.5:2.4b`, `exaone3.5:7.8b` | LG. 한영 특화. Ko-LLM 리더보드 상위권. |
| **Phi-4** | `phi4:14b` | Microsoft. 14B 효율·품질. |

### 용도별 추천 (한 줄)

- **한국어·다국어 우선**: exaone3.5 (2.4b / 7.8b), qwen3 (8b / 14b)
- **속도·경량 우선**: exaone3.5:2.4b, mistral:7b, qwen3:4b
- **추론·구조화 출력 우선**: llama3.1:8b, qwen3:8b/14b, deepseek-r1 (해당 사이즈)
- **최고 품질(리소스 여유)**: llama3.3:70b, qwen3:32b

Ollama 라이브러리 최신 목록은 `ollama list` 및 [Ollama Library](https://ollama.com/library) 참고.

### 우리 3개 복잡도(SIMPLE / MEDIUM / COMPLEX) 추천

위 리스트 기준으로, **이 프로젝트**에 넣을 때 추천 조합은 아래 두 가지다.

| 구분 | 1순위 (한국어·타협점) | 2순위 (다국어·2026 리스트) |
|------|------------------------|----------------------------|
| **SIMPLE** | `exaone3.5:2.4b` | `qwen3:4b` |
| **MEDIUM** | `exaone3.5:7.8b` | `qwen3:8b` |
| **COMPLEX** | `exaone3.5:7.8b` | `qwen3:8b` 또는 `qwen3:14b` |

- **1순위**: 한영 특화(EXAONE 3.5)로 통일. SIMPLE만 2.4b로 속도 유지, MEDIUM/COMPLEX는 7.8b로 트리·평가 품질 확보. 지금 application.conf 타협점 구성과 동일.
- **2순위**: Qwen3로 통일. 100+ 언어, 2026 리스트 8B/14B 상위 반영. COMPLEX만 14b로 올리면 품질 더 높음.

**정리**: 한국어·한영 일관성 쓰려면 **1순위**, 다국어·벤치 상위 쓰려면 **2순위**.

---

## 2~3B vs 7~8B: 정량·정성 한계 차이

같은 프롬프트·작업이라도 파라미터 규모에 따라 “할 수 있는 것”과 “한계”가 달라진다. 2~3B와 7~8B를 비교해 정리한다.

### 정량적 차이

| 항목 | 2~3B급 | 7~8B급 |
|------|--------|--------|
| **파라미터 수** | 약 2~3B | 약 7~8B (2.5~4배) |
| **모델 파일 크기** | 약 1.5~2GB | 약 4~5GB |
| **추론 시 RAM** | 약 2~4GB | 약 6~10GB |
| **동일 입력 대비 응답 시간** | 상대적으로 짧음 (수 초) | 2~4배 길어지는 경우 많음 |
| **컨텍스트(입력) 길이** | 4K~8K 토큰에서 성능 유지 | 8K~32K까지 실사용 가능한 경우 많음 |
| **벤치마크 점수** | MMLU 등 50% 전후 많음 | 60~70%대 많음 (모델·과제에 따라 상이) |

- **정량 요약**: 2~3B는 작고 빠르지만, 7~8B는 메모리·시간을 더 쓰는 대신 수치상 성능(정확도·벤치마크)이 전반적으로 높다.

### 정성적 차이 (실제 쓰일 때 느껴지는 한계)

| 관점 | 2~3B급 | 7~8B급 |
|------|--------|--------|
| **지시 따르기** | 짧고 단순한 지시는 잘 따름. “여러 단계를 반드시 모두 수행해라” 같은 복합 지시는 누락·단순화하기 쉬움. | 복합·다단계 지시를 더 안정적으로 따르는 편. |
| **구조화 출력** | JSON 등 구조는 만들지만, 필드 누락·잘못된 플레이스홀더(예: `{{parent}}` 대신 `{{echo}}`)가 나오기 쉬움. | 스키마·플레이스홀더 규칙을 더 정확히 지키는 경향. |
| **순차 vs 병렬** | “순차면 한 루트의 children으로”, “병렬이면 여러 루트” 같은 뉘앙스를 놓치고, 루트를 여러 개로 나누거나 순서를 잘못 세우기 쉬움. | 순차/병렬 구분과 트리 구조를 더 일관되게 맞추는 편. |
| **다국어(한국어)** | 영어 위주 학습 비중이 큰 모델은 한국어 길문·복문 이해가 떨어져, 앞부분만 반영하고 뒷부분을 무시하는 식으로 단순화하기 쉬움. | 다국어·한국어 비중이 있는 7~8B는 문장 전체와 의도를 더 잘 반영. |
| **추론·계획** | 한두 단계 수준의 계획은 가능. 3단계 이상·조건 분기·예외 처리까지 포함한 계획은 불완전하거나 단순화되기 쉽다. | 여러 단계·조건을 포함한 계획을 더 완전한 형태로 만드는 경향. |
| **환각·일관성** | 존재하지 않는 필드명·레이어명을 쓰거나, 한 번 정한 규칙을 뒤에서 바꾸는 등 일관성 문제가 더 자주 보임. | 같은 프롬프트에서 규칙과 필드 사용이 더 일관됨. |

- **정성 요약**: “트리 구성이 온전하지 않다”는 느낌(순차가 깨짐, 플레이스홀더 오타, 단계 누락)은 2~3B에서 더 자주 나오고, 7~8B는 같은 작업에서 구조·규칙을 더 잘 지키는 편이다.

### 이 프로젝트에 적용했을 때

- **트리 생성(COMPLEX)**  
  - 2~3B: “한 줄기(echo → toUpperCase → addPrefix)” + `{{parent}}`까지 맞추기는 어렵고, 루트 2개·잘못된 플레이스홀더가 나오기 쉬움.  
  - 7~8B: 같은 자연어 요청이라도 한 줄기 체인과 `{{parent}}` 사용을 더 안정적으로 맞추는 편.
- **평가·비교(MEDIUM)**  
  - 2~3B: 짧은 평가는 가능하나, 긴 결과·여러 조건을 동시에 고려하면 놓치는 경우 있음.  
  - 7~8B: 판단 기준을 더 꼼꼼히 따르는 경향.
- **단순 추출(SIMPLE)**  
  - 2~3B로도 충분한 경우 많음. 여기서는 속도·리소스 이점이 큼.

정리하면, **정량적으로는 속도·메모리에서 2~3B가 유리하고, 정성적으로는 “온전한 트리·규칙 준수”는 7~8B가 한계치가 더 높다**고 보면 된다.

---

## 현재 환경(48GB RAM, Apple M4 Pro)에서 온전히 뽑을 수 있는 모델

Ollama는 Mac에서 통합 메모리를 쓰므로, 48GB면 아래 크기까지 **스왑 없이** 여유 있게 돌릴 수 있다.

| 규모 | 예시 모델 | 예상 사용 메모리 | 이 환경에서 |
|------|-----------|------------------|-------------|
| 2~3B | gemma2:2b, exaone3.5:2.4b | ~2GB | 매우 여유 |
| 7B | mistral:7b | ~4~5GB | 여유 |
| 8B | llama3.1:8b, exaone3.5:7.8b, qwen3:8b | ~5~6GB | 여유 |
| 14B | qwen3:14b | ~8~10GB | 온전히 가능 |
| 32B | llama3.1:32b (Q4 등) | ~18~20GB | 온전히 가능 |
| 70B | llama3.1:70b (Q4) | ~40GB 전후 | 가능하나 한 번에 하나만, 추론은 상대적으로 느림 |

**정리**: 이 스펙이면 **7~8B는 당연히, 14B·32B도 온전히** 뽑을 수 있다. 트리 구성을 안정적으로 맞추려면 COMPLEX에 **exaone3.5:7.8b** 또는 **llama3.1:8b**를 쓰면 되고, 더 여유 있게 쓰고 싶으면 **qwen3:14b**까지 갈 수 있다.

---

## 작업별 특성 분석

### SIMPLE 작업
**작업**: `validateQueryFeasibility`, `extractParameters`
**특성**:
- 짧은 프롬프트 (100-500 토큰)
- 구조화된 JSON 출력 (간단한 스키마)
- 빠른 응답 필요 (실시간 처리)
- 높은 빈도 호출 (모든 요청마다 실행)

**요구사항**:
- 빠른 추론 속도
- 정확한 JSON 형식 준수
- 낮은 리소스 사용

### MEDIUM 작업
**작업**: `evaluateResult`, `compareExecutions`
**특성**:
- 중간 프롬프트 (500-2000 토큰)
- 판단과 비교 작업
- 중간 빈도 호출

**요구사항**:
- 좋은 판단 능력
- 컨텍스트 이해
- 균형잡힌 성능/속도

### COMPLEX 작업
**작업**: `createExecutionTree`, `suggestRetryStrategy`
**특성**:
- 긴 프롬프트 (2000-8000+ 토큰)
- 복잡한 추론과 계획 수립
- 많은 컨텍스트 처리
- 낮은 빈도 호출 (하지만 중요)

**요구사항**:
- 강력한 추론 능력
- 긴 컨텍스트 처리
- 구조화된 복잡한 출력

## 2026년 최신 모델 추천

### SIMPLE 작업용 모델

#### 1순위: **SmolLM2:1.7B** (또는 360M)
- **이유**: 
  - 초고속 추론 (135M-1.7B 파라미터)
  - 구조화된 출력에 최적화
  - 8K 컨텍스트로 충분
  - 메모리 효율적 (1.8GB)
- **사용 모델**: `smollm2:1.7b` 또는 `smollm2:360m`
- **예상 속도**: 매우 빠름 (100-300ms)
- **정확도**: 간단한 작업에 충분

#### 2순위: **Phi-3:3.8B**
- **이유**:
  - Microsoft의 경량 모델
  - 작은 크기지만 성능 우수
  - 구조화된 출력 지원
- **사용 모델**: `phi3:3.8b`
- **예상 속도**: 빠름 (200-500ms)

#### 3순위: **Mistral 7B**
- **이유**:
  - 빠른 추론 (20-30% 더 빠름)
  - 범용적 성능
- **사용 모델**: `mistral:7b`
- **예상 속도**: 빠름 (300-600ms)

### MEDIUM 작업용 모델

#### 1순위: **Llama 3.1:8B**
- **이유**:
  - 범용 작업에 최적화
  - 우수한 판단 능력
  - 8GB RAM으로 실행 가능
  - 안정적인 성능
- **사용 모델**: `llama3.1:8b`
- **예상 속도**: 중간 (500ms-2s)
- **정확도**: 높음

#### 2순위: **Qwen3:8B**
- **이유**:
  - 추론 성능 우수
  - 다국어 지원
  - 현재 사용 중인 모델과 일관성
- **사용 모델**: `qwen3:8b`
- **예상 속도**: 중간 (600ms-2s)

#### 3순위: **Mistral 7B**
- **이유**:
  - 빠른 속도
  - 효율적인 리소스 사용
- **사용 모델**: `mistral:7b`

### COMPLEX 작업용 모델

#### 1순위: **Qwen3:14B** (또는 8B)
- **이유**:
  - 복잡한 추론에 강함
  - 긴 컨텍스트 처리 (41K 토큰)
  - 구조화된 출력 생성 능력
  - 현재 사용 중인 모델과 일관성
- **사용 모델**: `qwen3:14b` (또는 `qwen3:8b`)
- **예상 속도**: 느림 (2-10s)
- **정확도**: 매우 높음

#### 2순위: **Llama 3.1:70B** (리소스 여유 시)
- **이유**:
  - 매우 강력한 추론 능력
  - 엔터프라이즈급 성능
  - 깊은 분석 가능
- **사용 모델**: `llama3.1:70b`
- **예상 속도**: 매우 느림 (5-30s)
- **리소스**: 32GB+ RAM 필요

#### 3순위: **Llama 3.1:8B**
- **이유**:
  - 리소스 제약 시 좋은 절충안
  - 여전히 좋은 성능
- **사용 모델**: `llama3.1:8b`

## 최종 추천 구성

### 구성 A: 한국어·타협점 (권장) — 2026 리스트 1순위
```
SIMPLE:  exaone3.5:2.4b    (속도, 한영)
MEDIUM:  exaone3.5:7.8b    (평가·직접답변 품질)
COMPLEX: exaone3.5:7.8b    (트리 구성 안정)
```
- 필요: `ollama pull exaone3.5:2.4b` + `ollama pull exaone3.5:7.8b`

### 구성 B: 다국어·2026 리스트 2순위
```
SIMPLE:  qwen3:4b          (경량, 100+ 언어)
MEDIUM:  qwen3:8b          (범용)
COMPLEX: qwen3:8b          또는 qwen3:14b (추론 강화)
```

### 구성 C: 속도 극대화
```
SIMPLE:  exaone3.5:2.4b    또는 smollm2:1.7b
MEDIUM:  mistral:7b
COMPLEX: qwen3:8b
```

### 구성 D: 성능 극대화 (리소스 여유 시)
```
SIMPLE:  exaone3.5:2.4b
MEDIUM:  llama3.1:8b       또는 qwen3:8b
COMPLEX: qwen3:14b         또는 llama3.3:70b
```

## 모델별 리소스 요구사항

| 모델 | RAM 필요 | 디스크 | 추론 속도 |
|------|----------|--------|-----------|
| smollm2:360m | 1GB | 726MB | 매우 빠름 |
| smollm2:1.7b | 2GB | 1.8GB | 매우 빠름 |
| phi3:3.8b | 4GB | ~2GB | 빠름 |
| mistral:7b | 8GB | ~4GB | 빠름 |
| llama3.1:8b | 8GB | ~4.5GB | 중간 |
| qwen3:8b | 8GB | ~5GB | 중간 |
| qwen3:14b | 16GB | ~8GB | 느림 |
| llama3.1:70b | 32GB+ | ~40GB | 매우 느림 |

## 선택 가이드

1. **리소스가 제한적** → 구성 B (속도 우선)
2. **균형잡힌 성능** → 구성 A (권장)
3. **최고 성능 필요** → 구성 C (성능 우선)
4. **현재 구성 유지** → 구성 D (일관성)

## 마이그레이션 고려사항

- 각 모델은 다른 출력 형식을 가질 수 있으므로 테스트 필요
- 프롬프트 튜닝이 필요할 수 있음
- 모델별 성능 벤치마크 권장
